{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the assignment, you will use data sourced from the Open Database of Addresses (ODA) provided courtesy of Statistics Canada. You can find the link to the database here: https://www.statcan.gc.ca/en/lode/databases/oda. \n",
    "\n",
    "This data is provided under the terms of the [Open Government License - Canada](https://open.canada.ca/en/open-government-licence-canada). \n",
    "\n",
    "You can find the data as a zipfile in D2L with the assignment, and you should unpack this and run it in the same directory as this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "First, explore your data and the conversion task that you will be working with. In the cell below:\n",
    "\n",
    "Compare the conversion of the files to parquet using different compression algorithms. What is the difference between using no compression, snappy, and gzip? Comment on processing times and approximately how much compression you observe has been achieved with this data.\n",
    "\n",
    "Note: You may have some issues running this with the brotli compression algorithm.\n",
    "\n",
    "Based on any sources you can find online (please include your references cite them approriately), how different would using brotli be? What would you expect in terms of processing time?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "import pandas as pd\n",
    "import random\n",
    "import time \n",
    "from pathlib import Path\n",
    "\n",
    "def convert_csv_to_parquet(src, dst, method):\n",
    "    # print(f\"Converting {src} to {dst}\")\n",
    "    df = pd.read_csv(src, low_memory=False)\n",
    "    df.to_parquet(dst, compression=method)\n",
    "    \n",
    "file_list = list(Path('address_data').glob('address_*.csv'))\n",
    "random.shuffle(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzip: 66.4325 seconds\n",
      "None: 36.4289 seconds\n",
      "snappy: 37.6192 seconds\n",
      "brotli: 82.3449 seconds\n"
     ]
    }
   ],
   "source": [
    "# MODIFIED - converted code into a fucntion that tests all compression algorithms \n",
    "methods = ['gzip', 'None', 'snappy', 'brotli']\n",
    "def file_conversion(file_list, methods):\n",
    "    for method in methods:\n",
    "        start_time = time.time()\n",
    "        for ind, src in enumerate(file_list):\n",
    "            dest = str(src.with_suffix('.parquet'))\n",
    "            convert_csv_to_parquet(src, dest, method)\n",
    "        print(f\"{method}: {time.time()-start_time:.4f} seconds\")\n",
    "file_conversion(file_list, methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:** \n",
    "\n",
    "Order of compression algorithm speeds:\n",
    "\n",
    "\n",
    "**gzip**: 66.4 seconds <br>\n",
    "**none**: 36.4 seconds <br>\n",
    "**snappy**: 37.6 seconds <br>\n",
    "**brotli**: 82.3 seconds <br>\n",
    "\n",
    "After some research online regardign these algorithms, this is why I believe they behaved in the way they did:\n",
    "\n",
    "\n",
    "**gzip**: <br>\n",
    "\n",
    "gzip compresses files using the DEFLATE algorithm. It is one of the most popular compresion algorithms since it has a compresison ratio around 95% for csv and json. It takes longer than snappy and no algorithm because it compresses the files so much.\n",
    "_Source_: https://kinsta.com/blog/enable-gzip-compression/#what-is-gzip-compression <br>\n",
    "\n",
    "**snappy**: <br>\n",
    "\n",
    "snappy is the default algorithm for the to_parquet function in pandas. It is made by Google and is meant to be faster than gzip while still providing some compression. This makes sense why it has a similar time to no compression algorithm.\n",
    "\n",
    "_Source_: https://github.com/google/snappy <br>\n",
    "\n",
    "**brotli**: <br>\n",
    "\n",
    "brotli is also made by Google and builds on the gzip compression algorithm to provide greater compression. This is why it takes so long to complete.\n",
    "\n",
    "_Source_: https://kinsta.com/blog/brotli-compression/#brotli-compression-vs-gzip-compression <br>\n",
    "\n",
    "**none**: <br>\n",
    "\n",
    "It makes sense that using no compression algorithm will be fastest since the files being converted to parquet are not being compressed at all. This means that although it is the fastest, the files take up the most space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Parallelize the code above using either processes or threads.\n",
    "\n",
    "Use comments reading `# MODIFIED` to indicate any code cells you have changed and `# NEW` to indicate any code cells you have added.\n",
    "\n",
    "With respect to the improvement in runtimes, do you think this is reasonably characteristic? What happens if you use a pool of threads or processes? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzip: 28.6826 seconds\n",
      "None: 22.0003 seconds\n",
      "snappy: 22.3294 seconds\n",
      "brotli: 34.7656 seconds\n"
     ]
    }
   ],
   "source": [
    "# NEW \n",
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "\n",
    "def convert_csv_to_parquet(src, dst, method):\n",
    "    df = pd.read_csv(src, low_memory=False)\n",
    "    return df.to_parquet(dst, compression=method)\n",
    "\n",
    "def thread_pool_func():\n",
    "    for method in ['gzip', 'None', 'snappy', 'brotli']:\n",
    "        start_time = time.time()\n",
    "        with Pool(4) as pool:\n",
    "            pool.starmap(convert_csv_to_parquet, [(f, str(f.with_suffix('.parquet')), method) for f in file_list])\n",
    "        print(f\"{method}: {time.time()-start_time:.4f} seconds\")\n",
    "\n",
    "thread_pool_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:** \n",
    "\n",
    "Using thread pools for multiprocessing, the Brotli and gzip compression algorithms have significantly reduced. However the snappy algorthim was about the same, if not a little slower. Using no algorithm was a little faster using thread pools.\n",
    "\n",
    "**gzip**: 28.7 seconds <br>\n",
    "**none**: 22.0 seconds <br>\n",
    "**snappy**: 22.3 seconds <br>\n",
    "**brotli**: 34.8 seconds <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Share one way in which you could improve the running of this test. You may consider (but are not limited to) your runtime environment, the test data provided, or the manner in which this test was run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way I could improve the performance of this test in terms of the time it takes to run is to optimize the amount of thread pools for the hardware I'm using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My PC has 8 cpu cores which means that using around 8 threads should improve the time of the test. I can utilize the code \"multiprocessing.cpu_count()\" in my code to implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzip: 23.7607 seconds\n",
      "None: 20.7990 seconds\n",
      "snappy: 20.9314 seconds\n",
      "brotli: 28.7523 seconds\n"
     ]
    }
   ],
   "source": [
    "# MODIFIED\n",
    "def convert_csv_to_parquet(src, dst, method):\n",
    "    df = pd.read_csv(src, low_memory=False)\n",
    "    return df.to_parquet(dst, compression=method)\n",
    "\n",
    "def thread_pool_func():\n",
    "    for method in ['gzip', 'None', 'snappy', 'brotli']:\n",
    "        start_time = time.time()\n",
    "        with Pool(multiprocessing.cpu_count()) as pool:\n",
    "            pool.starmap(convert_csv_to_parquet, [(f, str(f.with_suffix('.parquet')), method) for f in file_list])\n",
    "        print(f\"{method}: {time.time()-start_time:.4f} seconds\")\n",
    "\n",
    "thread_pool_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Improvement:**_\n",
    "\n",
    "**gzip**: 4.9sec <br>\n",
    "**none**: 1.2sec <br>\n",
    "**snappy**: 1.4sec <br>\n",
    "**brotli**: 6.0sec <br>\n",
    "\n",
    "We see that utilizing the appropriate amount of cores for the hardware the test is being run on improves the time of the test. We see that the \"heavier\" compression algorithms had more significant improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
